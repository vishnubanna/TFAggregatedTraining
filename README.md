# TFAggregatedTraining
Batch Size is often an important hyper parameter required fro obtaining good results for training on larger models. Tensorflow builds support for large batch sizes through tools like Distributed training, but for small scale systems this is not viable. This library provides a set of Optimizers and Batch Normalization layers to allow for Large Batch Size Simulation on Small scale GPU's
